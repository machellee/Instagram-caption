# -*- coding: utf-8 -*-
"""Lab 3.ipynb

Automatically generated by Colab.

"""

from datasets import load_dataset
import zipfile
import os

dataset = load_dataset("mrSoul7766/instagram_post_captions", split="train")

zip_path = "/instagram_images.zip"  # instagram zip file downloaded
extract_to = "/data"


with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_to)

print(f"âœ… Files extracted to {extract_to}")

from datasets import load_dataset, Dataset
from PIL import Image, UnidentifiedImageError
import os

def load_indexed_image_caption_dataset(captions_dataset, image_folder):

    image_paths = []
    captions = []

    for idx, row in enumerate(captions_dataset):
        # Construct image filename based on index
        image_filename = f"{idx:04d}.jpg"  # e.g., 0000.jpg
        image_path = os.path.join(image_folder, image_filename)

        # Check if the image file exists and is valid
        if not os.path.exists(image_path):
            continue

        try:
            with Image.open(image_path) as img:
                img.verify()  # Verify it can be opened
        except (UnidentifiedImageError, IOError, OSError):
            continue  # Skip corrupted or unreadable images

        # Add valid pair
        image_paths.append(image_path)
        captions.append(row["caption"])  # Adjust if your column name is different

    # Return as a Hugging Face dataset
    return Dataset.from_dict({
        "image_path": image_paths,
        "caption": captions
    })

from datasets import load_dataset

# Your local image folder
image_folder = "/data/images"

# Filter and match
filtered_dataset = load_indexed_image_caption_dataset(dataset, image_folder)

print(f"âœ… Final dataset size: {len(filtered_dataset)}")
print(filtered_dataset[0])

from PIL import Image, UnidentifiedImageError
from transformers import BlipProcessor
import torch

def prepare_blip_dataset(captions_dataset, image_folder, processor):
    processed = []

    for idx, row in enumerate(captions_dataset):
        image_filename = f"{idx:04d}.jpg"
        image_path = os.path.join(image_folder, image_filename)

        # Check if file exists and is valid
        if not os.path.exists(image_path):
            continue

        try:
            image = Image.open(image_path).convert("RGB")
        except (UnidentifiedImageError, IOError, OSError):
            continue

        caption = row["caption"]

        # Tokenize image and caption
        encoding = processor(
            images=image,
            text=caption,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=64,
        )

        # Set labels = input_ids (for training loss)
        encoding["labels"] = encoding["input_ids"]

        # Move tensors from batch dim [1, x] to scalar [x]
        encoding = {k: v.squeeze(0) for k, v in encoding.items()}

        processed.append(encoding)

    return processed

from transformers import BlipProcessor
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
processed_dataset = prepare_blip_dataset(filtered_dataset, "/data/images", processor)

"""### Test Before Fine Tuning"""

from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
from IPython.display import display
import torch

# 1. Load the model and processor
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model.eval()

# 2. Load your image
image_path = "poit.jpg"  # <-- replace with your image path
raw_image = Image.open(image_path).convert("RGB")

# 3. Preprocess and move to model device
inputs = processor(images=raw_image, return_tensors="pt").to(model.device)

# 4. Generate caption
with torch.no_grad():
    generated_ids = model.generate(**inputs, max_length=50)
    caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

    plt.imshow(raw_image)
    plt.axis("off")
    plt.title(f"Caption: {caption}", fontsize=12)
    plt.show()

from torch.utils.data import Dataset

class BlipFineTuneDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

train_dataset = BlipFineTuneDataset(processed_dataset)

from torch.utils.data import DataLoader

train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)

from transformers import BlipForConditionalGeneration

model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
model.train()

optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

num_epochs = 3

for epoch in range(num_epochs):
    print(f"Epoch {epoch+1}/{num_epochs}")
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}

        outputs = model(**batch)
        loss = outputs.loss

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        print(f"Loss: {loss.item()}")

model.save_pretrained("./blip-instagram-caption-model")
processor.save_pretrained("./blip-instagram-caption-model")

print("âœ… Fine-tuned model saved to ./blip-instagram-caption-model")

"""### Quick Test for Comparison"""

from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration

# Load model and processor
processor = BlipProcessor.from_pretrained("./blip-instagram-caption-model")
model = BlipForConditionalGeneration.from_pretrained("./blip-instagram-caption-model").to(device)

# Load image
image = Image.open("000001090023250615.jpg").convert("RGB")

inputs = processor(image, return_tensors="pt").to(device)
out = model.generate(**inputs, max_new_tokens=50)
caption = processor.decode(out[0], skip_special_tokens=True)

display(image)

print("ðŸ“¸ Caption:", caption)

"""### Upload Model"""

from transformers import BlipForConditionalGeneration, BlipProcessor

# Load your fine-tuned model and processor
model = BlipForConditionalGeneration.from_pretrained("./blip-instagram-caption-model")
processor = BlipProcessor.from_pretrained("./blip-instagram-caption-model")

# Your Hugging Face info
hf_user_name = "mle1"  # <- Replace with your actual HF username
repo_name = "instagram_caption_blip"
model_name = f"{hf_user_name}/{repo_name}"
commit_m = "Upload fine-tuned BLIP model for Instagram captions"

# Push both model and processor
model.push_to_hub(model_name, commit_message=commit_m)
processor.push_to_hub(model_name, commit_message=commit_m)

"""### Evaluation"""

# Load Fine Tuned Model
from transformers import BlipForConditionalGeneration, BlipProcessor

test_model = BlipForConditionalGeneration.from_pretrained("mle1/instagram_caption_blip",use_safetensors=True).to(device)
test_processor = BlipProcessor.from_pretrained("mle1/instagram_caption_blip")

base_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)
base_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")

from datasets import load_dataset

dataset = load_dataset("kkcosmos/instagram-images-with-captions", split="train[:50]")

import torch
device = "cuda" if torch.cuda.is_available() else "cpu"

def generate_caption(image, processor, model):
    inputs = processor(image, return_tensors="pt").to(device)
    with torch.no_grad():
        out = model.generate(**inputs, max_new_tokens=30, do_sample=True)
    return processor.decode(out[0], skip_special_tokens=True)

results = []

for example in dataset:
    image = example["image"]
    ground_truth = example["caption"]

    base_caption = generate_caption(image, base_processor, base_model)
    finetuned_caption = generate_caption(image, test_processor, test_model)

    results.append({
        "ground_truth": ground_truth,
        "base_caption": base_caption,
        "finetuned_caption": finetuned_caption
    })

import random
from IPython.display import display

random.seed(508)
# Example: 10 random integers between 1 and 100

random_numbers= random.sample(range(50), 10)

# Loop through the selected random indices
for idx in random_numbers:
    sample = results[idx]

    print(f"ðŸ“· Sample #{idx}")
    display(dataset[idx]["image"])
    print("Ground Truth Caption: ", sample["ground_truth"])
    print("ðŸ–¼ï¸ Base Model Caption: ", sample["base_caption"])
    print("ðŸ“¸ Fine-Tuned Model Caption: ", sample["finetuned_caption"])
    print("-" * 80)

"""### Further Fine Tuning"""

from datasets import load_dataset
import zipfile
import os
## More Fine Tuning
zip_path = "data/archive.zip"  # instagram zip file downloaded
extract_to = "/data"


with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_to)

print(f"âœ… Files extracted to {extract_to}")

import os
import pandas as pd
from PIL import Image
from torch.utils.data import Dataset

class InstagramDataset(Dataset):
    def __init__(self, csv_path, image_folder, processor):
        self.data = pd.read_csv(csv_path)
        self.data = self.data.dropna(subset=["Caption"])
        self.data = self.data[self.data["Caption"].str.strip().astype(bool)]

        self.image_folder = image_folder
        self.processor = processor

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]

        image_filename = row["Image File"].strip()
        if not image_filename.lower().endswith(".jpg"):
            image_filename += ".jpg"
        caption = row["Caption"].strip()

        image_path = os.path.join(self.image_folder, image_filename)

        image = Image.open(image_path).convert("RGB")

        # Preprocess for BLIP
        encoding = self.processor(
            images=image,
            text=caption,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=64,
        )

        encoding["labels"] = encoding["input_ids"]
        encoding = {k: v.squeeze(0) for k, v in encoding.items()}
        return encoding

from torch.utils.data import DataLoader

ft_dataset = InstagramDataset(
    csv_path="data/instagram_data/captions_csv.csv",
    image_folder="data/instagram_data/",
    processor=test_processor
)

dataloader = DataLoader(ft_dataset, batch_size=8, shuffle=True)

device = "cuda" if torch.cuda.is_available() else "cpu"

num_epochs = 2
test_model.train()
optimizer = torch.optim.AdamW(test_model.parameters(), lr=5e-5)

for epoch in range(num_epochs):
    print(f"Epoch {epoch+1}/{num_epochs}")
    for batch in dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}

        outputs = test_model(**batch)
        loss = outputs.loss

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        print(f"Loss: {loss.item()}")

test_model.save_pretrained("./blip-instagram-caption-model2")
test_processor.save_pretrained("./blip-instagram-caption-model2")

print("âœ… Fine-tuned model saved to ./blip-instagram-caption-model2")

from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration

# Load model and processor
processor = BlipProcessor.from_pretrained("./blip-instagram-caption-model2")
model = BlipForConditionalGeneration.from_pretrained("./blip-instagram-caption-model2").to(device)

# Load image
image = Image.open("000001090025250615.jpg").convert("RGB")

inputs = processor(image, return_tensors="pt").to(device)
out = model.generate(**inputs, max_new_tokens=50)
caption = processor.decode(out[0], skip_special_tokens=True)

display(image)

print("ðŸ“¸ Caption:", caption)

results2 = []

for example in dataset:
    image = example["image"]
    ground_truth = example["caption"]

    base_caption = generate_caption(image, base_processor, base_model)
    finetuned_caption1 = generate_caption(image, test_processor, test_model)
    finetuned_caption2 = generate_caption(image, processor, model)

    results2.append({
        "image": image,
        "ground_truth": ground_truth,
        "base_caption": base_caption,
        "finetuned_caption": finetuned_caption1,
        "finetuned_caption2": finetuned_caption2,

    })

import random
from IPython.display import display

random.seed(508)
# Example: 10 random integers between 1 and 100

random_numbers= random.sample(range(50), 10)

# Loop through the selected random indices
for idx in random_numbers:
    sample = results2[idx]

    print(f"ðŸ“· Sample #{idx}")
    display(dataset[idx]["image"])
    print("Ground Truth Caption: ", sample["ground_truth"])
    print("ðŸ–¼ï¸ Base Model Caption: ", sample["base_caption"])
    print("ðŸ“¸ Fine-Tuned Model Caption: ", sample["finetuned_caption"])
    print("ðŸ“¸ Fine-Tuned Model Caption 2: ", sample["finetuned_caption2"])

    print("-" * 80)

from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

def compute_clip_score(image, caption, model, processor):
    inputs = processor(text=[caption], images=image, return_tensors="pt", padding=True).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
        logits_per_image = outputs.logits_per_image  # shape [1,1]
    return logits_per_image.item()

base_scores = []
ft1_scores = []
ft2_scores = []

for entry in results2:
    image = entry["image"]

    base = compute_clip_score(image, entry["base_caption"], clip_model, clip_processor)
    ft1 = compute_clip_score(image, entry["finetuned_caption"], clip_model, clip_processor)
    ft2 = compute_clip_score(image, entry["finetuned_caption2"], clip_model, clip_processor)

    base_scores.append(base)
    ft1_scores.append(ft1)
    ft2_scores.append(ft2)

import numpy as np

print(f"ðŸ”¹ Base Model CLIPScore:      {np.mean(base_scores):.4f}")
print(f"ðŸ”¸ Fine-Tuned v1 CLIPScore:   {np.mean(ft1_scores):.4f}")
print(f"ðŸŸ£ Fine-Tuned v2 CLIPScore:   {np.mean(ft2_scores):.4f}")


# Get captions for comparison
base_captions = [r["base_caption"] for r in results2]
finetuned_captions1 = [r["finetuned_caption"] for r in results2]
ground_truths = [r["ground_truth"] for r in results2]
finetuned_captions2 = [r["finetuned_caption2"] for r in results2]

from bert_score import score

# Compare base model captions
P_base, R_base, F1_base = score(base_captions, ground_truths, lang="en")
print("ðŸ”¹ Base Model BERTScore F1:", F1_base.mean().item())

# Compare fine-tuned captions
P_ft, R_ft, F1_ft = score(finetuned_captions1, ground_truths, lang="en")
print("ðŸ”¸ Fine-Tuned Model BERTScore F1:", F1_ft.mean().item())

# Compare fine-tuned captions
P_ft, R_ft, F1_ft2 = score(finetuned_captions2, ground_truths, lang="en")
print("ðŸ”¸ Fine-Tuned2 Model BERTScore F1:", F1_ft2.mean().item())
